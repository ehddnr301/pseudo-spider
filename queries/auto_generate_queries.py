#!/usr/bin/env python3

import yaml
import json
import glob
import random
from pathlib import Path


def scan_generated_yaml_files():
    """generated_yaml í´ë”ì˜ ëª¨ë“  sources.yml íŒŒì¼ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤."""
    yaml_pattern = "../pseudolab/models/generated_yaml/*/sources.yml"
    yaml_files = glob.glob(yaml_pattern)

    datasets = {}

    for yaml_file in yaml_files:
        try:
            with open(yaml_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            for source in data.get("sources", []):
                source_name = source.get("name")

                for table in source.get("tables", []):
                    table_name = table.get("name")

                    # DataHub URN í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    dataset_urn = f"urn:li:dataset:(urn:li:dataPlatform:dbt,{source_name}.{source_name}.{table_name},PROD)"

                    # ê° ì»¬ëŸ¼ ì •ë³´ ì €ì¥
                    columns = []
                    for column in table.get("columns", []):
                        columns.append(
                            {
                                "name": column.get("name"),
                                "description": column.get("description", ""),
                            }
                        )

                    datasets[dataset_urn] = {
                        "source_name": source_name,
                        "table_name": table_name,
                        "table_description": table.get("description", ""),
                        "columns": columns,
                    }

        except Exception as e:
            print(f"âš ï¸  {yaml_file} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"ğŸ“ {len(datasets)}ê°œ ë°ì´í„°ì…‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    return datasets


def identify_column_types(columns):
    """ì»¬ëŸ¼ëª…ê³¼ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¬ëŸ¼ íƒ€ì…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    id_columns = []
    name_columns = []
    date_columns = []
    numeric_columns = []
    rating_columns = []
    location_columns = []

    for col in columns:
        col_name = col["name"].lower()
        col_desc = col["description"].lower()

        # ID ì»¬ëŸ¼
        if "id" in col_name or "ì‹ë³„ì" in col_desc or "ê³ ìœ " in col_desc:
            id_columns.append(col["name"])

        # ì´ë¦„ ê´€ë ¨ ì»¬ëŸ¼
        elif "name" in col_name or "ì´ë¦„" in col_desc or "ëª…ì¹­" in col_desc:
            name_columns.append(col["name"])

        # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼
        elif any(word in col_name for word in ["date", "time", "year"]) or any(
            word in col_desc for word in ["ë‚ ì§œ", "ì‹œê°„", "ì—°ë„"]
        ):
            date_columns.append(col["name"])

        # í‰ì  ê´€ë ¨ ì»¬ëŸ¼
        elif any(word in col_name for word in ["rating", "score", "stars"]) or any(
            word in col_desc for word in ["í‰ì ", "ì ìˆ˜", "ë³„ì "]
        ):
            rating_columns.append(col["name"])

        # ìœ„ì¹˜ ê´€ë ¨ ì»¬ëŸ¼
        elif any(
            word in col_name for word in ["address", "city", "country", "location"]
        ) or any(word in col_desc for word in ["ì£¼ì†Œ", "ë„ì‹œ", "êµ­ê°€", "ìœ„ì¹˜"]):
            location_columns.append(col["name"])

        # ìˆ«ì ê´€ë ¨ ì»¬ëŸ¼ (ê°€ê²©, ê¸‰ì—¬, ì”ì•¡ ë“±)
        elif any(
            word in col_name
            for word in ["price", "salary", "balance", "amount", "cost", "spent"]
        ) or any(word in col_desc for word in ["ê°€ê²©", "ê¸‰ì—¬", "ì”ì•¡", "ê¸ˆì•¡", "ë¹„ìš©"]):
            numeric_columns.append(col["name"])

    return {
        "id_columns": id_columns,
        "name_columns": name_columns,
        "date_columns": date_columns,
        "numeric_columns": numeric_columns,
        "rating_columns": rating_columns,
        "location_columns": location_columns,
    }


def generate_basic_queries(dataset_urn, dataset_info):
    """ê¸°ë³¸ì ì¸ ë°ì´í„° íƒìƒ‰ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    queries = []
    source_name = dataset_info["source_name"]
    table_name = dataset_info["table_name"]
    table_description = dataset_info["table_description"]

    # ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ
    queries.append(
        {
            "properties": {
                "name": f"{table_name} í…Œì´ë¸” ì „ì²´ ë°ì´í„° ìˆ˜ ì¡°íšŒ",
                "description": f"{table_description}ì˜ ì „ì²´ ë ˆì½”ë“œ ìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„° ê·œëª¨ íŒŒì•…",
                "statement": {
                    "value": f"SELECT COUNT(*) as total_count FROM dbt.{source_name}.{source_name}.{table_name};",
                    "language": "SQL",
                },
            },
            "subjects": [{"datasetUrn": dataset_urn}],
        }
    )

    return queries


def generate_aggregation_queries(dataset_urn, dataset_info, column_types):
    """ì§‘ê³„ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    queries = []
    source_name = dataset_info["source_name"]
    table_name = dataset_info["table_name"]

    # ê·¸ë£¹ë³„ ì§‘ê³„ ì¿¼ë¦¬
    for col in column_types["name_columns"] + column_types["location_columns"]:
        if col:
            queries.append(
                {
                    "properties": {
                        "name": f"{col}ë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„",
                        "description": f"{table_name} í…Œì´ë¸”ì—ì„œ {col}ë³„ ë°ì´í„° ìˆ˜ë¥¼ ì§‘ê³„í•˜ì—¬ ë¶„í¬ í˜„í™© íŒŒì•…",
                        "statement": {
                            "value": f"SELECT {col}, COUNT(*) as count FROM dbt.{source_name}.{source_name}.{table_name} GROUP BY {col} ORDER BY count DESC LIMIT 10;",
                            "language": "SQL",
                        },
                    },
                    "subjects": [{"datasetUrn": dataset_urn}],
                }
            )

    # ìˆ«ì ì»¬ëŸ¼ í†µê³„
    for col in column_types["numeric_columns"] + column_types["rating_columns"]:
        if col:
            queries.append(
                {
                    "properties": {
                        "name": f"{col} í†µê³„ ë¶„ì„",
                        "description": f"{table_name} í…Œì´ë¸”ì˜ {col} ê°’ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„(í‰ê· , ìµœëŒ€, ìµœì†Œê°’) ë¶„ì„",
                        "statement": {
                            "value": f"SELECT AVG({col}) as avg_value, MIN({col}) as min_value, MAX({col}) as max_value, COUNT(*) as total_count FROM dbt.{source_name}.{source_name}.{table_name} WHERE {col} IS NOT NULL;",
                            "language": "SQL",
                        },
                    },
                    "subjects": [{"datasetUrn": dataset_urn}],
                }
            )

    return queries


def generate_temporal_queries(dataset_urn, dataset_info, column_types):
    """ì‹œê°„ ê¸°ë°˜ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    queries = []
    source_name = dataset_info["source_name"]
    table_name = dataset_info["table_name"]

    for date_col in column_types["date_columns"]:
        if date_col:
            # ì—°ë„ë³„ íŠ¸ë Œë“œ
            if "year" in date_col.lower() or "ì—°ë„" in date_col.lower():
                queries.append(
                    {
                        "properties": {
                            "name": f"{date_col}ë³„ ë°ì´í„° íŠ¸ë Œë“œ ë¶„ì„",
                            "description": f"{table_name} í…Œì´ë¸”ì˜ {date_col}ë³„ ë°ì´í„° ë³€í™” ì¶”ì„¸ë¥¼ ë¶„ì„",
                            "statement": {
                                "value": f"SELECT {date_col}, COUNT(*) as count FROM dbt.{source_name}.{source_name}.{table_name} WHERE {date_col} IS NOT NULL GROUP BY {date_col} ORDER BY {date_col} DESC;",
                                "language": "SQL",
                            },
                        },
                        "subjects": [{"datasetUrn": dataset_urn}],
                    }
                )
            else:
                # ë‚ ì§œë³„ íŠ¸ë Œë“œ (ë…„ë„ ì¶”ì¶œ)
                queries.append(
                    {
                        "properties": {
                            "name": f"{date_col} ê¸°ë°˜ ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„",
                            "description": f"{table_name} í…Œì´ë¸”ì˜ {date_col}ì—ì„œ ì—°ë„ë¥¼ ì¶”ì¶œí•˜ì—¬ ì—°ë„ë³„ ë°ì´í„° íŠ¸ë Œë“œ ë¶„ì„",
                            "statement": {
                                "value": f"SELECT YEAR({date_col}) as year, COUNT(*) as count FROM dbt.{source_name}.{source_name}.{table_name} WHERE {date_col} IS NOT NULL GROUP BY YEAR({date_col}) ORDER BY year DESC;",
                                "language": "SQL",
                            },
                        },
                        "subjects": [{"datasetUrn": dataset_urn}],
                    }
                )

    return queries


def generate_top_bottom_queries(dataset_urn, dataset_info, column_types):
    """ìƒìœ„/í•˜ìœ„ ë­í‚¹ ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    queries = []
    source_name = dataset_info["source_name"]
    table_name = dataset_info["table_name"]

    # í‰ì ì´ë‚˜ ìˆ«ì ê°’ ê¸°ì¤€ ìƒìœ„/í•˜ìœ„
    ranking_cols = column_types["rating_columns"] + column_types["numeric_columns"]
    name_cols = column_types["name_columns"]

    if ranking_cols and name_cols:
        ranking_col = ranking_cols[0]
        name_col = name_cols[0]

        queries.append(
            {
                "properties": {
                    "name": f"{ranking_col} ê¸°ì¤€ ìƒìœ„ í•­ëª© ë¶„ì„",
                    "description": f"{table_name} í…Œì´ë¸”ì—ì„œ {ranking_col} ê°’ì´ ë†’ì€ ìƒìœ„ í•­ëª©ë“¤ì„ ì¡°íšŒ",
                    "statement": {
                        "value": f"SELECT {name_col}, {ranking_col} FROM dbt.{source_name}.{source_name}.{table_name} WHERE {ranking_col} IS NOT NULL ORDER BY {ranking_col} DESC LIMIT 10;",
                        "language": "SQL",
                    },
                },
                "subjects": [{"datasetUrn": dataset_urn}],
            }
        )

    return queries


def generate_all_queries():
    """ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸš€ ìë™ ì¿¼ë¦¬ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")

    # ë°ì´í„°ì…‹ ìŠ¤ìº”
    datasets = scan_generated_yaml_files()

    all_queries = []

    for dataset_urn, dataset_info in datasets.items():
        print(
            f"ğŸ“Š {dataset_info['source_name']}.{dataset_info['table_name']} ì²˜ë¦¬ ì¤‘..."
        )

        # ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜
        column_types = identify_column_types(dataset_info["columns"])

        # ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì¿¼ë¦¬ ìƒì„±
        queries = []
        queries.extend(generate_basic_queries(dataset_urn, dataset_info))
        queries.extend(
            generate_aggregation_queries(dataset_urn, dataset_info, column_types)
        )
        queries.extend(
            generate_temporal_queries(dataset_urn, dataset_info, column_types)
        )
        queries.extend(
            generate_top_bottom_queries(dataset_urn, dataset_info, column_types)
        )

        # ëœë¤í•˜ê²Œ ì¼ë¶€ ì¿¼ë¦¬ë§Œ ì„ íƒ (ë„ˆë¬´ ë§ì•„ì§€ì§€ ì•Šë„ë¡)
        if len(queries) > 3:
            queries = random.sample(queries, min(3, len(queries)))

        all_queries.extend(queries)
        print(f"   âœ… {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„±ë¨")

    # JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = "auto_generated_queries.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_queries, f, indent=2, ensure_ascii=False)

    print(
        f"\nğŸ‰ ì´ {len(all_queries)}ê°œì˜ ì¿¼ë¦¬ê°€ ìƒì„±ë˜ì–´ '{output_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!"
    )
    return all_queries


if __name__ == "__main__":
    generate_all_queries()
