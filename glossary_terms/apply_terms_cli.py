#!/usr/bin/env python3

import subprocess
import json
import os
import requests
import time
import yaml
import glob
from pathlib import Path

# DataHub ì„¤ì •
DATAHUB_GMS_URL = "http://localhost:8080"


def load_glossary_terms():
    """glossary.yml íŒŒì¼ì—ì„œ ëª¨ë“  ìš©ì–´ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    glossary_file = "glossary.yml"
    terms = {}

    try:
        with open(glossary_file, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)

        for node in data.get("nodes", []):
            for term in node.get("terms", []):
                term_name = term.get("name")
                term_description = term.get("description", "")
                terms[term_name] = term_description

        print(f"ðŸ“š ìš©ì–´ì‚¬ì „ì—ì„œ {len(terms)}ê°œ ìš©ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return terms

    except Exception as e:
        print(f"âŒ glossary.yml ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def scan_generated_yaml_files():
    """generated_yaml í´ë”ì˜ ëª¨ë“  sources.yml íŒŒì¼ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤."""
    yaml_pattern = "../pseudolab/models/generated_yaml/*/sources.yml"
    yaml_files = glob.glob(yaml_pattern)

    datasets = {}

    for yaml_file in yaml_files:
        try:
            with open(yaml_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            for source in data.get("sources", []):
                source_name = source.get("name")

                for table in source.get("tables", []):
                    table_name = table.get("name")

                    # DataHub URN í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    dataset_urn = f"urn:li:dataset:(urn:li:dataPlatform:dbt,{source_name}.{source_name}.{table_name},PROD)"

                    # ê° ì»¬ëŸ¼ ì •ë³´ ì €ìž¥
                    columns = []
                    for column in table.get("columns", []):
                        columns.append(
                            {
                                "name": column.get("name"),
                                "description": column.get("description", ""),
                            }
                        )

                    datasets[dataset_urn] = {
                        "source_name": source_name,
                        "table_name": table_name,
                        "columns": columns,
                    }

        except Exception as e:
            print(f"âš ï¸  {yaml_file} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"ðŸ“ {len(datasets)}ê°œ ë°ì´í„°ì…‹ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    return datasets


def create_term_matching_rules():
    """ìš©ì–´ë³„ ë§¤ì¹­ ê·œì¹™ì„ ì •ì˜í•©ë‹ˆë‹¤."""
    return {
        "ê³ ìœ  ì‹ë³„ìž": ["ì‹ë³„ìž", "ì‹ë³„", "ID", "id", "ê³ ìœ "],
        "ì™¸ëž˜í‚¤ ì°¸ì¡°": ["ì°¸ì¡°", "ì™¸ëž˜í‚¤", "FK", "fk", "key"],
        "ê°œì²´ëª…": ["ì´ë¦„", "ëª…ì¹­", "name", "Name"],
        "ì„±ëª…": ["ì´ë¦„", "ì„±ëª…", "name", "Name", "fname", "lname"],
        "ë³„ëª… ë° ì•½ì–´": ["ë³„ëª…", "nickname", "alias", "ì•½ì–´"],
        "ìœ„ì¹˜ ì •ë³´": ["ìœ„ì¹˜", "ìž¥ì†Œ", "location", "address", "ì£¼ì†Œ"],
        "ì§€ì—­ ì½”ë“œ": ["ì§€ì—­", "ì½”ë“œ", "code", "region"],
        "ë‚ ì§œ ì •ë³´": ["ë‚ ì§œ", "date", "time", "ì‹œê°„"],
        "ì„¤ë¦½ì¼": ["ì„¤ë¦½", "ì°½ë¦½", "established", "founded"],
        "ë°©ë¬¸ ì‹œê°„": ["ë°©ë¬¸", "visit", "time"],
        "ìž”ì•¡": ["ìž”ì•¡", "balance", "ê¸ˆì•¡"],
        "ë¹„ìš© ë° ê°€ê²©": ["ë¹„ìš©", "ê°€ê²©", "price", "cost", "fee"],
        "ê¸‰ì—¬": ["ê¸‰ì—¬", "salary", "pay", "wage"],
        "ë³´í—˜ ê¸ˆì•¡": ["ë³´í—˜", "insurance"],
        "ë‚˜ì´": ["ë‚˜ì´", "age", "ì—°ë ¹"],
        "ì„±ë³„": ["ì„±ë³„", "sex", "gender"],
        "ì‹ ì²´ ì •ë³´": ["ì‹ ì²´", "í‚¤", "ëª¸ë¬´ê²Œ", "height", "weight"],
        "í•™ë…„": ["í•™ë…„", "grade", "level"],
        "ì „ê³µ": ["ì „ê³µ", "major", "í•™ê³¼"],
        "ì§€ë„êµìˆ˜": ["ì§€ë„êµìˆ˜", "êµìˆ˜", "advisor", "professor"],
        "í•™ìƒ ìˆ˜": ["í•™ìƒìˆ˜", "í•™ìƒ", "student"],
        "ì†Œì†": ["ì†Œì†", "ê¸°ê´€", "organization"],
        "ìž¥ë¥´ ë° ì¹´í…Œê³ ë¦¬": ["ìž¥ë¥´", "ì¹´í…Œê³ ë¦¬", "genre", "category", "type"],
        "í‰ì  ë° í‰ê°€": ["í‰ì ", "í‰ê°€", "rating", "score"],
        "ë¯¸ë””ì–´ íŒŒì¼ ì •ë³´": ["íŒŒì¼", "ë¯¸ë””ì–´", "format"],
        "ì°½ìž‘ìž ì •ë³´": ["ì°½ìž‘ìž", "ê°ë…", "director", "creator"],
        "ì‹œì¦Œ ì„±ì ": ["ì‹œì¦Œ", "ì„±ì ", "season"],
        "ìŠ¹ë¥ ": ["ìŠ¹ë¥ ", "ìŠ¹ë¦¬", "win"],
        "ê²½ê¸°ìž¥ë³„ ì„±ì ": ["ê²½ê¸°ìž¥", "í™ˆ", "ì›ì •", "home", "away"],
        "ê±°ë¦¬": ["ê±°ë¦¬", "distance"],
        "ì†Œì…œ ê´€ê³„": ["ê´€ê³„", "ì¹œêµ¬", "friend", "relation"],
        "ì†Œìœ  ê´€ê³„": ["ì†Œìœ ", "ë³´ìœ ", "has", "own"],
        "ë¶„ë¥˜ ê´€ê³„": ["ë¶„ë¥˜", "ì¹´í…Œê³ ë¦¬", "class"],
        "ì‹œí€€ìŠ¤ ê´€ë¦¬": ["ì‹œí€€ìŠ¤", "sequence", "ìˆœì„œ"],
        "API ì‹ë³„ìž": ["API", "api"],
        "ì§€ë¶ˆ ë°©ë²• ì½”ë“œ": ["ì§€ë¶ˆ", "ê²°ì œ", "payment"],
    }


def match_terms_with_columns(terms, datasets):
    """ìš©ì–´ì™€ ì»¬ëŸ¼ì„ ë§¤ì¹­í•˜ì—¬ TERM_DATASET_MAPPINGì„ ìƒì„±í•©ë‹ˆë‹¤."""
    matching_rules = create_term_matching_rules()
    term_dataset_mapping = {}

    print("\nðŸ” ìš©ì–´ì™€ ì»¬ëŸ¼ ë§¤ì¹­ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")

    for term_name, term_description in terms.items():
        matching_keywords = matching_rules.get(term_name, [])
        matched_datasets = []

        print(f"\nðŸ“‹ '{term_name}' ìš©ì–´ ë§¤ì¹­ ì¤‘...")
        print(f"   í‚¤ì›Œë“œ: {matching_keywords}")

        for dataset_urn, dataset_info in datasets.items():
            for column in dataset_info["columns"]:
                column_desc = column["description"].lower()
                column_name = column["name"].lower()

                # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                for keyword in matching_keywords:
                    if keyword.lower() in column_desc or keyword.lower() in column_name:

                        if dataset_urn not in matched_datasets:
                            matched_datasets.append(dataset_urn)
                            print(
                                f"   âœ… ë§¤ì¹­: {dataset_info['source_name']}.{dataset_info['table_name']}.{column['name']} -> {column['description']}"
                            )
                        break

        if matched_datasets:
            term_dataset_mapping[term_name] = matched_datasets
            print(f"   ðŸ“Š ì´ {len(matched_datasets)}ê°œ ë°ì´í„°ì…‹ê³¼ ë§¤ì¹­ë¨")
        else:
            print(f"   âš ï¸  ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ì…‹ ì—†ìŒ")

    return term_dataset_mapping


def get_glossary_term_urn_by_name(term_name):
    """ìš©ì–´ëª…ìœ¼ë¡œ ì‹¤ì œ DataHub URNì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        search_payload = {
            "input": term_name,
            "entity": "glossaryTerm",
            "start": 0,
            "count": 10,
        }

        response = requests.post(
            f"{DATAHUB_GMS_URL}/entities?action=search",
            headers={"Content-Type": "application/json"},
            json=search_payload,
        )

        if response.status_code == 200:
            data = response.json()
            entities = data.get("value", {}).get("entities", [])

            # ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” ìš©ì–´ ì°¾ê¸°
            for entity in entities:
                matched_fields = entity.get("matchedFields", [])
                for field in matched_fields:
                    if field.get("name") == "name" and field.get("value") == term_name:
                        return entity.get("entity")

            # ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
            if entities:
                return entities[0].get("entity")

        return None

    except Exception as e:
        print(f"ìš©ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def create_glossary_terms_json(term_urn, output_file):
    """ìš©ì–´ ì ìš©ì„ ìœ„í•œ JSON íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    aspect_data = {
        "terms": [{"urn": term_urn}],
        "auditStamp": {
            "time": int(time.time() * 1000),  # í˜„ìž¬ ì‹œê°„ì„ ë°€ë¦¬ì´ˆë¡œ
            "actor": "urn:li:corpuser:dwlee",
        },
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(aspect_data, f, indent=2, ensure_ascii=False)


def apply_term_to_dataset(dataset_urn, term_urn, term_name):
    """CLIë¥¼ ì‚¬ìš©í•´ì„œ ìš©ì–´ë¥¼ ë°ì´í„°ì…‹ì— ì ìš©í•©ë‹ˆë‹¤."""
    try:
        # ìž„ì‹œ JSON íŒŒì¼ ìƒì„±
        temp_json = "temp_aspect.json"
        create_glossary_terms_json(term_urn, temp_json)

        # CLI ëª…ë ¹ì–´ ì‹¤í–‰
        cmd = [
            "datahub",
            "put",
            "aspect",
            "--urn",
            dataset_urn,
            "--aspect",
            "glossaryTerms",
            "--aspect-data",
            temp_json,
        ]

        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        env = os.environ.copy()
        env["DATAHUB_GMS_URL"] = DATAHUB_GMS_URL

        result = subprocess.run(cmd, capture_output=True, text=True, env=env)

        # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_json):
            os.remove(temp_json)

        if result.returncode == 0:
            print(f"      âœ… ì„±ê³µ: '{term_name}' ìš©ì–´ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
            return True
        else:
            print(f"      âŒ ì‹¤íŒ¨: {result.stderr}")
            return False

    except Exception as e:
        print(f"      âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ðŸš€ ìžë™í™”ëœ Glossary Terms ì ìš©ì„ ì‹œìž‘í•©ë‹ˆë‹¤...\n")

    # 1. ìš©ì–´ì‚¬ì „ ë¡œë“œ
    terms = load_glossary_terms()
    if not terms:
        print("âŒ ìš©ì–´ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„°ì…‹ ìŠ¤ìº”
    datasets = scan_generated_yaml_files()
    if not datasets:
        print("âŒ ë°ì´í„°ì…‹ ìŠ¤ìº” ì‹¤íŒ¨. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 3. ìš©ì–´-ì»¬ëŸ¼ ë§¤ì¹­
    term_dataset_mapping = match_terms_with_columns(terms, datasets)

    if not term_dataset_mapping:
        print("âŒ ë§¤ì¹­ë˜ëŠ” ìš©ì–´-ë°ì´í„°ì…‹ ì¡°í•©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. DataHubì— ì ìš©
    print(f"\nðŸ”§ DataHubì— ìš©ì–´ ì ìš©ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")
    success_count = 0
    error_count = 0

    for term_name, dataset_urns in term_dataset_mapping.items():
        print(f"\nðŸ“Œ ìš©ì–´ '{term_name}' ì²˜ë¦¬ ì¤‘...")

        # ìš©ì–´ URN ì¡°íšŒ
        term_urn = get_glossary_term_urn_by_name(term_name)
        if not term_urn:
            print(f"    âš ï¸  ìš©ì–´ '{term_name}'ì˜ URNì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            error_count += len(dataset_urns)
            continue

        print(f"    ìš©ì–´ URN: {term_urn}")

        # ê° ë°ì´í„°ì…‹ì— ìš©ì–´ ì ìš©
        for dataset_urn in dataset_urns:
            dataset_info = datasets.get(dataset_urn, {})
            dataset_name = f"{dataset_info.get('source_name', '')}.{dataset_info.get('table_name', '')}"
            print(f"    ðŸ“Š {dataset_name}ì— ì ìš© ì¤‘...")

            if apply_term_to_dataset(dataset_urn, term_urn, term_name):
                success_count += 1
            else:
                error_count += 1

    print("\n" + "=" * 60)
    print(f"ðŸŽ‰ ìž‘ì—… ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}ê±´")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê±´")
    print(f"ðŸ“Š ì´ ì‹œë„: {success_count + error_count}ê±´")
    print(
        f"ðŸ“ˆ ì„±ê³µë¥ : {success_count/(success_count + error_count)*100:.1f}%"
        if (success_count + error_count) > 0
        else "0%"
    )


if __name__ == "__main__":
    main()
